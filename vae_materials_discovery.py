#%%
#!/usr/bin/env python3
"""
# Variational Autoencoder for Materials Discovery

## Workshop: Machine Learning for Materials Science

### Project 1: Materials Representation Learning using Variational Autoencoders

**Learning Objectives:**
- Understand VAE principles and implementation
- Learn materials representation learning
- Implement property prediction from latent space
- Explore the latent space for materials discovery

**Dataset:** Formation Energy Dataset (Materials Project subset)
- ~5,000 materials with formation energies (processed from raw data)
- Composition-based features (elemental properties, stoichiometry)
- Target: Formation energy per atom (eV/atom)

**VAE Task:** Learn a compact latent representation of materials that captures:
- Chemical composition patterns
- Formation energy relationships
- Materials similarity metrics

**Data Consistency Notes:**
- This script automatically handles raw structure data by creating sample features
- For real data processing, run `preprocess_data.py` first to extract composition features
- The script gracefully handles different data formats and column names
- Robust validation ensures data quality before VAE training

---
"""
#%%
# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import random
import os
import json
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

# Deep learning libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# Plotting style
plt.style.use('default')
sns.set_theme(style="whitegrid")
sns.set_palette("husl")

print("üß† Variational Autoencoder for Materials Discovery")
print("üéØ Goal: Learn meaningful representations of materials for discovery")

# Create output directories
def create_output_directories():
    """Create organized output directories for materials, images, and models."""
    directories = ['materials', 'images', 'models']
    for directory in directories:
        Path(directory).mkdir(exist_ok=True)
    print("üìÅ Created output directories: materials/, images/, models/")

# Create directories at start
create_output_directories()

"""
## 1. Introduction to Variational Autoencoders for Materials

Variational Autoencoders (VAEs) are powerful generative models that learn:

### Key Concepts:
- **Encoder**: Maps materials features to latent distribution parameters (Œº, œÉ¬≤)
- **Latent Space**: Low-dimensional probabilistic representation of materials
- **Decoder**: Reconstructs materials features from latent codes
- **Reparameterization Trick**: Enables gradient flow through random sampling

### VAE Loss Function:
- **Reconstruction Loss**: How well can we reconstruct input materials?
- **KL Divergence**: How close is the latent distribution to a prior (N(0,I))?
- **Property Loss**: Can we predict materials properties from latent codes?

### Applications for Materials Science:
- **Materials Discovery**: Sample new materials from latent space
- **Property Prediction**: Predict properties from compact representations
- **Materials Similarity**: Use latent distance as materials similarity
- **Feature Engineering**: Use latent codes as features for downstream tasks
"""

#%%
def create_sample_formation_energy_dataset(n_samples=5000):
    """
    Create a sample formation energy dataset for workshop purposes.
    In practice, this would come from Materials Project or similar database.
    """
    print("üìä Creating sample formation energy dataset...")
    
    # Set random seed for reproducibility
    np.random.seed(42)
    
    # Define some common elements and their properties
    elements = ['H', 'Li', 'C', 'N', 'O', 'F', 'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'K', 'Ca', 
                'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn']
    
    # Elemental properties (simplified)
    atomic_numbers = [1, 3, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 19, 20, 
                     22, 23, 24, 25, 26, 27, 28, 29, 30]
    electronegativities = [2.20, 0.98, 2.55, 3.04, 3.44, 3.98, 0.93, 1.31, 1.61, 1.90, 2.19, 2.58, 
                          3.16, 0.82, 1.00, 1.54, 1.63, 1.66, 1.55, 1.83, 1.88, 1.91, 1.90, 1.65]
    atomic_radii = [0.31, 1.67, 0.67, 0.56, 0.48, 0.42, 1.90, 1.45, 1.18, 1.11, 0.98, 0.88, 
                   0.79, 2.43, 1.94, 1.76, 1.71, 1.66, 1.61, 1.56, 1.52, 1.49, 1.45, 1.42]
    
    # Create feature vectors for each material
    features = []
    formation_energies = []
    
    for i in range(n_samples):
        # Random number of elements (1-4)
        n_elements = np.random.choice([1, 2, 3, 4], p=[0.1, 0.5, 0.3, 0.1])
        
        # Select random elements
        selected_elements = np.random.choice(len(elements), size=n_elements, replace=False)
        
        # Create composition fractions
        fractions = np.random.dirichlet(np.ones(n_elements))
        
        # Calculate weighted features
        feature_vector = np.zeros(100)  # 100-dimensional feature space
        
        # Basic elemental features
        for j, elem_idx in enumerate(selected_elements):
            weight = fractions[j]
            
            # Add elemental contributions
            feature_vector[elem_idx] = weight  # Element presence
            feature_vector[24 + elem_idx] = weight * atomic_numbers[elem_idx]  # Weighted atomic number
            feature_vector[48 + elem_idx] = weight * electronegativities[elem_idx]  # Weighted electronegativity
            
        # Global composition features
        feature_vector[72] = n_elements  # Number of elements
        feature_vector[73] = np.mean([atomic_numbers[idx] for idx in selected_elements])  # Average atomic number
        feature_vector[74] = np.std([atomic_numbers[idx] for idx in selected_elements])  # Std atomic number
        feature_vector[75] = np.mean([electronegativities[idx] for idx in selected_elements])  # Average electronegativity
        feature_vector[76] = np.std([electronegativities[idx] for idx in selected_elements])  # Std electronegativity
        feature_vector[77] = np.mean([atomic_radii[idx] for idx in selected_elements])  # Average atomic radius
        
        # Add some random features
        feature_vector[78:] = np.random.normal(0, 0.1, 22)
        
        features.append(feature_vector)
        
        # Simulate formation energy based on composition
        # This is a simplified model - real formation energies are much more complex
        base_energy = -2.0  # Base formation energy
        
        # Energy contributions from different factors
        electronegativity_effect = -0.5 * feature_vector[75]  # Higher electronegativity = more stable
        complexity_penalty = 0.3 * n_elements  # More elements = less stable generally
        random_noise = np.random.normal(0, 0.5)  # Random variation
        
        formation_energy = base_energy + electronegativity_effect + complexity_penalty + random_noise
        formation_energies.append(formation_energy)
    
    # Create DataFrame
    feature_columns = [f'feature_{i}' for i in range(100)]
    df = pd.DataFrame(features, columns=feature_columns)
    df['e_form_per_atom'] = formation_energies
    
    # Add some categorical information for analysis
    df['n_elements'] = [int(row[72]) for _, row in df.iterrows()]
    
    print(f"‚úÖ Created dataset with {len(df)} materials")
    print(f"üìä Formation energy range: {df['e_form_per_atom'].min():.3f} to {df['e_form_per_atom'].max():.3f} eV/atom")
    
    return df

def load_formation_energy_dataset():
    """
    Load formation energy dataset for VAE training.
    """
    data_dir = Path("../data") # common data directory
    
    # Try to load processed dataset first
    processed_file = data_dir / "day1_formation_energy_processed.csv"
    if processed_file.exists():
        try:
            print("üìÇ Loading processed formation energy dataset...")
            df = pd.read_csv(processed_file)
            print(f"‚úÖ Loaded {len(df)} materials with processed features")
            print(f"üìä Dataset shape: {df.shape}")
            print(f"üéØ Formation energy range: {df['e_form_per_atom'].min():.3f} to {df['e_form_per_atom'].max():.3f} eV/atom")
            return df
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading processed dataset: {e}")
    
    if not data_dir.exists():
        print("‚ö†Ô∏è Workshop data not found. Creating sample dataset...")
        return create_sample_formation_energy_dataset()
    
    try:
        # Load the prepared formation energy dataset
        df = pd.read_csv(data_dir / "day1_formation_energy.csv")
        print(f"‚úÖ Loaded raw data with {len(df)} materials")
        
        # Check if this is raw structure data or processed features
        if 'structure' in df.columns and 'e_form' in df.columns:
            print("‚ö†Ô∏è Raw structure data detected. Creating processed dataset for VAE...")
            return create_sample_formation_energy_dataset()
        
        # Check for correct column name
        if 'e_form' in df.columns and 'e_form_per_atom' not in df.columns:
            df = df.rename(columns={'e_form': 'e_form_per_atom'})
            print("üîß Renamed 'e_form' column to 'e_form_per_atom'")
        
        # Display basic info
        print(f"üìä Dataset shape: {df.shape}")
        if 'e_form_per_atom' in df.columns:
            print(f"üéØ Formation energy range: {df['e_form_per_atom'].min():.3f} to {df['e_form_per_atom'].max():.3f} eV/atom")
        
        return df
        
    except FileNotFoundError:
        print("‚ö†Ô∏è Prepared dataset not found. Creating sample dataset...")
        return create_sample_formation_energy_dataset()


#%%
"""
## 2. Data Loading and Exploration
"""

# Load the dataset
df = load_formation_energy_dataset()

def validate_dataset(df):
    """Validate the dataset for VAE training."""
    print("\nüîç Validating dataset for VAE training...")
    
    issues = []
    warnings = []
    
    # Check required columns
    if 'e_form_per_atom' not in df.columns:
        issues.append("Missing target column 'e_form_per_atom'")
    
    # Check for numerical features
    numerical_cols = [col for col in df.columns 
                     if col != 'e_form_per_atom' and pd.api.types.is_numeric_dtype(df[col])]
    
    if len(numerical_cols) == 0:
        issues.append("No numerical features found for VAE training")
    elif len(numerical_cols) < 5:
        warnings.append(f"Only {len(numerical_cols)} numerical features found - may limit VAE performance")
    
    # Check for missing values
    missing_values = df.isnull().sum().sum()
    if missing_values > 0:
        warnings.append(f"{missing_values} missing values detected")
    
    # Check target distribution
    if 'e_form_per_atom' in df.columns:
        target_std = df['e_form_per_atom'].std()
        if target_std < 0.01:
            warnings.append("Very low variance in target variable")
        
        # Check for outliers
        q1 = df['e_form_per_atom'].quantile(0.25)
        q3 = df['e_form_per_atom'].quantile(0.75)
        iqr = q3 - q1
        outliers = ((df['e_form_per_atom'] < (q1 - 1.5 * iqr)) | 
                   (df['e_form_per_atom'] > (q3 + 1.5 * iqr))).sum()
        
        if outliers > len(df) * 0.1:
            warnings.append(f"{outliers} potential outliers detected ({outliers/len(df)*100:.1f}%)")
    
    # Check dataset size
    if len(df) < 100:
        issues.append("Dataset too small for reliable VAE training (< 100 samples)")
    elif len(df) < 1000:
        warnings.append("Small dataset may limit VAE performance (< 1000 samples)")
    
    # Print results
    if issues:
        print("‚ùå Critical Issues Found:")
        for issue in issues:
            print(f"   - {issue}")
        return False
    
    if warnings:
        print("‚ö†Ô∏è Warnings:")
        for warning in warnings:
            print(f"   - {warning}")
    
    if not issues and not warnings:
        print("‚úÖ Dataset validation passed!")
    elif not issues:
        print("‚úÖ Dataset validation passed with warnings")
    
    print(f"üìä Dataset summary: {len(df)} samples, {len(numerical_cols)} features")
    return True

# Validate the dataset
if not validate_dataset(df):
    print("\n‚ùå Dataset validation failed. Please check the data.")
    print("üí° Suggestion: Run the data preprocessing script first.")
else:
    print("\n‚úÖ Dataset ready for VAE training")

def explore_dataset(df):
    """Explore the formation energy dataset."""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # Formation energy distribution
    axes[0, 0].hist(df['e_form_per_atom'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    axes[0, 0].set_xlabel('Formation Energy (eV/atom)')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].set_title('Distribution of Formation Energies')
    axes[0, 0].grid(True, alpha=0.3)
    
    # Cumulative distribution
    sorted_energies = np.sort(df['e_form_per_atom'])
    y_vals = np.arange(1, len(sorted_energies) + 1) / len(sorted_energies)
    axes[0, 1].plot(sorted_energies, y_vals, linewidth=2)
    axes[0, 1].set_xlabel('Formation Energy (eV/atom)')
    axes[0, 1].set_ylabel('Cumulative Probability')
    axes[0, 1].set_title('Cumulative Distribution')
    axes[0, 1].grid(True, alpha=0.3)
    
    # Feature statistics
    feature_cols = [col for col in df.columns if col.startswith('feature_') or 
                   (col not in ['e_form_per_atom', 'reduced_formula', 'n_elements'] and 
                    pd.api.types.is_numeric_dtype(df[col]))][:10]
    
    if feature_cols:
        feature_means = df[feature_cols].mean()
        axes[0, 2].bar(range(len(feature_means)), feature_means.values)
        axes[0, 2].set_xlabel('Feature Index')
        axes[0, 2].set_ylabel('Mean Value')
        axes[0, 2].set_title('Sample Feature Means')
        axes[0, 2].tick_params(axis='x', rotation=45)
    else:
        axes[0, 2].text(0.5, 0.5, 'No numerical\nfeatures found', 
                       ha='center', va='center', transform=axes[0, 2].transAxes)
        axes[0, 2].set_title('Feature Analysis')
    axes[0, 2].grid(True, alpha=0.3)
    
    # Formation energy vs complexity
    if 'n_elements' in df.columns:
        box_data = [df[df['n_elements'] == n]['e_form_per_atom'].values 
                   for n in sorted(df['n_elements'].unique()) if len(df[df['n_elements'] == n]) > 0]
        box_labels = [str(n) for n in sorted(df['n_elements'].unique()) 
                     if len(df[df['n_elements'] == n]) > 0]
        
        if box_data:
            axes[1, 0].boxplot(box_data, labels=box_labels)
            axes[1, 0].set_xlabel('Number of Elements')
            axes[1, 0].set_ylabel('Formation Energy (eV/atom)')
            axes[1, 0].set_title('Formation Energy by Composition Complexity')
        else:
            axes[1, 0].text(0.5, 0.5, 'No composition\ncomplexity data', 
                           ha='center', va='center', transform=axes[1, 0].transAxes)
            axes[1, 0].set_title('Composition Complexity')
    else:
        axes[1, 0].text(0.5, 0.5, 'No n_elements\ncolumn found', 
                       ha='center', va='center', transform=axes[1, 0].transAxes)
        axes[1, 0].set_title('Composition Complexity')
    
    # Feature correlation heatmap (sample)
    if feature_cols:
        corr_data = df[feature_cols + ['e_form_per_atom']].corr()['e_form_per_atom'][:-1]
        
        axes[1, 1].barh(range(len(corr_data)), corr_data.values)
        axes[1, 1].set_yticks(range(len(corr_data)))
        axes[1, 1].set_yticklabels([col.replace('_', '\n') for col in corr_data.index], fontsize=8)
        axes[1, 1].set_xlabel('Correlation with Formation Energy')
        axes[1, 1].set_title('Feature Correlations')
        axes[1, 1].grid(True, alpha=0.3)
    else:
        axes[1, 1].text(0.5, 0.5, 'No features for\ncorrelation analysis', 
                       ha='center', va='center', transform=axes[1, 1].transAxes)
        axes[1, 1].set_title('Feature Correlations')
    
    # Data quality assessment
    axes[1, 2].text(0.1, 0.9, f"Dataset Quality Assessment:", fontsize=12, weight='bold', 
                   transform=axes[1, 2].transAxes)
    
    quality_info = []
    quality_info.append(f"Total samples: {len(df)}")
    quality_info.append(f"Total features: {len(df.columns) - 1}")
    
    # Check for missing values
    missing_values = df.isnull().sum().sum()
    quality_info.append(f"Missing values: {missing_values}")
    
    # Check feature types
    numeric_features = len([col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])])
    quality_info.append(f"Numeric features: {numeric_features}")
    
    # Target statistics
    if 'e_form_per_atom' in df.columns:
        target_mean = df['e_form_per_atom'].mean()
        target_std = df['e_form_per_atom'].std()
        quality_info.append(f"Target mean: {target_mean:.3f}")
        quality_info.append(f"Target std: {target_std:.3f}")
    
    for i, info in enumerate(quality_info):
        axes[1, 2].text(0.1, 0.8 - i*0.1, info, fontsize=10, 
                       transform=axes[1, 2].transAxes)
    
    axes[1, 2].set_xlim(0, 1)
    axes[1, 2].set_ylim(0, 1)
    axes[1, 2].axis('off')
    
    plt.tight_layout()
    plt.savefig('images/formation_energy_dataset_exploration.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Print summary statistics
    print("\nüìä Dataset Statistics:")
    print(f"Total materials: {len(df)}")
    print(f"Features: {len(df.columns) - 1}")
    print(f"Formation energy range: {df['e_form_per_atom'].min():.3f} to {df['e_form_per_atom'].max():.3f} eV/atom")
    print(f"Mean formation energy: {df['e_form_per_atom'].mean():.3f} ¬± {df['e_form_per_atom'].std():.3f} eV/atom")
    
    if 'n_elements' in df.columns:
        print(f"\nComposition complexity:")
        complexity_counts = df['n_elements'].value_counts().sort_index()
        for n_elem, count in complexity_counts.items():
            print(f"  {n_elem} elements: {count} materials")
    
    # Check for data quality issues
    quality_issues = []
    if df.isnull().sum().sum() > 0:
        quality_issues.append("Missing values detected")
    if len(df.columns) < 5:
        quality_issues.append("Very few features available")
    if df['e_form_per_atom'].std() < 0.1:
        quality_issues.append("Low variance in target variable")
    
    if quality_issues:
        print(f"\n‚ö†Ô∏è Data Quality Issues:")
        for issue in quality_issues:
            print(f"  - {issue}")
    else:
        print(f"\n‚úÖ Data quality looks good!")

explore_dataset(df)

# Prepare data for VAE
def prepare_vae_data(df, test_size=0.2, val_size=0.1):
    """
    Prepare and split data for VAE training.
    """
    # Check if target column exists
    target_col = 'e_form_per_atom'
    if target_col not in df.columns:
        print("‚ö†Ô∏è Target column 'e_form_per_atom' not found in DataFrame")
        print(f"Available columns: {list(df.columns)}")
        raise ValueError(f"Target column '{target_col}' not found")
    
    # Separate features and target
    feature_cols = [col for col in df.columns if col != target_col and not col.startswith('n_elements')]
    
    # Ensure we have numerical features
    if not feature_cols:
        print("‚ö†Ô∏è No feature columns found. This might be raw structure data.")
        print("Available columns:", list(df.columns))
        raise ValueError("No suitable feature columns found for VAE training")
    
    X = df[feature_cols].values
    y = df[target_col].values
    
    print(f"üìè Feature matrix shape: {X.shape}")
    print(f"üéØ Target vector shape: {y.shape}")
    print(f"üîß Using {len(feature_cols)} features: {feature_cols[:5]}{'...' if len(feature_cols) > 5 else ''}")
    
    # Check for non-numeric data
    if not np.issubdtype(X.dtype, np.number):
        print("‚ö†Ô∏è Non-numeric features detected. Converting to numeric.")
        X = np.array(X, dtype=float)
    
    # Train-test split
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=None
    )
    
    # Train-validation split
    val_size_adj = val_size / (1 - test_size)
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_size_adj, random_state=42
    )
    
    print(f"\nüìÇ Data splits:")
    print(f"   Training: {X_train.shape[0]} samples")
    print(f"   Validation: {X_val.shape[0]} samples")
    print(f"   Test: {X_test.shape[0]} samples")
    
    # Scale features (important for VAE)
    feature_scaler = StandardScaler()
    X_train_scaled = feature_scaler.fit_transform(X_train)
    X_val_scaled = feature_scaler.transform(X_val)
    X_test_scaled = feature_scaler.transform(X_test)
    
    # Scale targets
    target_scaler = StandardScaler()
    y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
    y_val_scaled = target_scaler.transform(y_val.reshape(-1, 1)).flatten()
    y_test_scaled = target_scaler.transform(y_test.reshape(-1, 1)).flatten()
    
    print(f"\nüîß Scaling statistics:")
    print(f"   Features - Mean: {X_train_scaled.mean():.3f}, Std: {X_train_scaled.std():.3f}")
    print(f"   Targets - Mean: {y_train_scaled.mean():.3f}, Std: {y_train_scaled.std():.3f}")
    
    return {
        'X_train': X_train_scaled, 'X_val': X_val_scaled, 'X_test': X_test_scaled,
        'y_train': y_train_scaled, 'y_val': y_val_scaled, 'y_test': y_test_scaled,
        'feature_scaler': feature_scaler, 'target_scaler': target_scaler,
        'feature_names': feature_cols
    }

#%%
# Prepare the data
data = prepare_vae_data(df)
n_features = data['X_train'].shape[1]
print(f"\n‚úÖ Data preparation complete. Feature dimension: {n_features}")

#%%
"""
## 3. VAE Architecture

We'll implement a VAE with the following architecture:
- **Encoder**: Maps materials features to latent distribution parameters (Œº, log_œÉ¬≤)
- **Reparameterization**: Samples latent codes using the reparameterization trick
- **Decoder**: Reconstructs materials features from latent codes
- **Property Predictor**: Predicts formation energy from latent codes
"""

class MaterialsVAE(nn.Module):
    """
    Variational Autoencoder for materials representation learning.
    
    Architecture:
    - Encoder: features -> latent distribution (Œº, log_œÉ¬≤)
    - Decoder: latent -> reconstructed features
    - Property predictor: latent -> formation energy
    """
    
    def __init__(self, input_dim, latent_dim=10, hidden_dims=[128, 64]):
        super(MaterialsVAE, self).__init__()
        
        self.input_dim = input_dim
        self.latent_dim = latent_dim
        self.hidden_dims = hidden_dims
        
        # Encoder network
        encoder_layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            encoder_layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            prev_dim = hidden_dim
        
        self.encoder = nn.Sequential(*encoder_layers)
        
        # Latent space parameters
        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)
        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)
        
        # Decoder network
        decoder_layers = []
        prev_dim = latent_dim
        
        for hidden_dim in reversed(hidden_dims):
            decoder_layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            prev_dim = hidden_dim
        
        decoder_layers.append(nn.Linear(prev_dim, input_dim))
        self.decoder = nn.Sequential(*decoder_layers)
        
        # Property predictor
        self.property_predictor = nn.Sequential(
            nn.Linear(latent_dim, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 1)
        )
    
    def encode(self, x):
        """Encode input to latent distribution parameters."""
        h = self.encoder(x)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        """Sample from latent distribution using reparameterization trick."""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        """Decode latent code to reconstructed features."""
        return self.decoder(z)
    
    def predict_property(self, z):
        """Predict material property from latent code."""
        return self.property_predictor(z)
    
    def forward(self, x):
        """Full forward pass through VAE."""
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon_x = self.decode(z)
        property_pred = self.predict_property(z)
        return recon_x, property_pred, mu, logvar, z

# Create model
model = MaterialsVAE(
    input_dim=n_features,
    latent_dim=10,
    hidden_dims=[min(128, n_features//2), min(64, n_features//4)] if n_features > 32 else [32, 16]
).to(device)

print(f"\nüß† Created VAE model with {sum(p.numel() for p in model.parameters()):,} parameters")
print(f"üìê Latent dimension: {model.latent_dim}")
print(f"üîß Hidden dimensions: {model.hidden_dims}")
print(f"üìè Input dimension: {n_features}")

#%%
"""
## 4. VAE Training Loop
"""

def vae_loss_function(recon_x, x, predicted_property, true_property, mu, logvar, 
                      beta=1.0, property_weight=1.0):
    """
    VAE loss function combining reconstruction, KL divergence, and property prediction.
    """
    # Reconstruction loss (MSE)
    recon_loss = F.mse_loss(recon_x, x, reduction='sum')
    
    # KL divergence loss
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    
    # Property prediction loss
    property_loss = F.mse_loss(predicted_property.squeeze(), true_property, reduction='sum')
    
    # Total loss
    total_loss = recon_loss + beta * kl_loss + property_weight * property_loss
    
    return total_loss, recon_loss, kl_loss, property_loss

#%%


# Training parameters
BATCH_SIZE = 64
LEARNING_RATE = 1e-3
EPOCHS = 100
BETA_START = 0.0
BETA_END = 1.0
PROPERTY_WEIGHT = 10.0

# Create data loaders
train_dataset = TensorDataset(
    torch.FloatTensor(data['X_train']),
    torch.FloatTensor(data['y_train'])
)
val_dataset = TensorDataset(
    torch.FloatTensor(data['X_val']),
    torch.FloatTensor(data['y_val'])
)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

# Optimizer and scheduler
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)

def train_epoch(model, train_loader, optimizer, epoch, total_epochs):
    """Train the VAE for one epoch."""
    model.train()
    total_loss = 0
    total_recon_loss = 0
    total_kl_loss = 0
    total_property_loss = 0
    
    # Beta annealing
    beta = BETA_START + (BETA_END - BETA_START) * epoch / total_epochs
    
    for batch_idx, (x, y) in enumerate(train_loader):
        x, y = x.to(device), y.to(device)
        
        optimizer.zero_grad()
        
        # Forward pass
        recon_x, predicted_property, mu, logvar, z = model(x)
        
        # Compute loss
        loss, recon_loss, kl_loss, property_loss = vae_loss_function(
            recon_x, x, predicted_property, y, mu, logvar,
            beta=beta, property_weight=PROPERTY_WEIGHT
        )
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Accumulate losses
        total_loss += loss.item()
        total_recon_loss += recon_loss.item()
        total_kl_loss += kl_loss.item()
        total_property_loss += property_loss.item()
    
    return {
        'total_loss': total_loss / len(train_loader.dataset),
        'recon_loss': total_recon_loss / len(train_loader.dataset),
        'kl_loss': total_kl_loss / len(train_loader.dataset),
        'property_loss': total_property_loss / len(train_loader.dataset),
        'beta': beta
    }

def validate_epoch(model, val_loader, epoch, total_epochs):
    """Validate the VAE for one epoch."""
    model.eval()
    total_loss = 0
    total_recon_loss = 0
    total_kl_loss = 0
    total_property_loss = 0
    
    beta = BETA_START + (BETA_END - BETA_START) * epoch / total_epochs
    
    with torch.no_grad():
        for x, y in val_loader:
            x, y = x.to(device), y.to(device)
            
            recon_x, predicted_property, mu, logvar, z = model(x)
            
            loss, recon_loss, kl_loss, property_loss = vae_loss_function(
                recon_x, x, predicted_property, y, mu, logvar,
                beta=beta, property_weight=PROPERTY_WEIGHT
            )
            
            total_loss += loss.item()
            total_recon_loss += recon_loss.item()
            total_kl_loss += kl_loss.item()
            total_property_loss += property_loss.item()
    
    return {
        'total_loss': total_loss / len(val_loader.dataset),
        'recon_loss': total_recon_loss / len(val_loader.dataset),
        'kl_loss': total_kl_loss / len(val_loader.dataset),
        'property_loss': total_property_loss / len(val_loader.dataset)
    }

# Training loop
print(f"\nüöÄ Starting VAE training for {EPOCHS} epochs...")
print(f"üéõÔ∏è Configuration:")
print(f"   Batch size: {BATCH_SIZE}")
print(f"   Learning rate: {LEARNING_RATE}")
print(f"   Beta annealing: {BETA_START} ‚Üí {BETA_END}")
print(f"   Property weight: {PROPERTY_WEIGHT}")

# Storage for training history
history = {
    'train_loss': [], 'val_loss': [],
    'train_recon': [], 'val_recon': [],
    'train_kl': [], 'val_kl': [],
    'train_property': [], 'val_property': [],
    'beta': []
}

#%%
best_val_loss = float('inf')
patience_counter = 0
patience = 20

for epoch in range(EPOCHS):
    # Training
    train_metrics = train_epoch(model, train_loader, optimizer, epoch, EPOCHS)
    
    # Validation
    val_metrics = validate_epoch(model, val_loader, epoch, EPOCHS)
    
    # Update learning rate
    scheduler.step(val_metrics['total_loss'])
    
    # Store history
    history['train_loss'].append(train_metrics['total_loss'])
    history['val_loss'].append(val_metrics['total_loss'])
    history['train_recon'].append(train_metrics['recon_loss'])
    history['val_recon'].append(val_metrics['recon_loss'])
    history['train_kl'].append(train_metrics['kl_loss'])
    history['val_kl'].append(val_metrics['kl_loss'])
    history['train_property'].append(train_metrics['property_loss'])
    history['val_property'].append(val_metrics['property_loss'])
    history['beta'].append(train_metrics['beta'])
    
    # Early stopping
    if val_metrics['total_loss'] < best_val_loss:
        best_val_loss = val_metrics['total_loss']
        patience_counter = 0
        # Save best model
        if not Path('models').exists():
            Path('models').mkdir()
        model_path = Path('models/best_vae_model.pth')
        print(f"üíæ Saving best model to {model_path}")
        torch.save(model.state_dict(), model_path)
    else:
        patience_counter += 1
    
    # Print progress
    if epoch % 10 == 0 or epoch == EPOCHS - 1:
        print(f"Epoch {epoch+1:3d}/{EPOCHS} | "
              f"Train Loss: {train_metrics['total_loss']:.4f} | "
              f"Val Loss: {val_metrics['total_loss']:.4f} | "
              f"Beta: {train_metrics['beta']:.3f} | "
              f"Property Loss: {val_metrics['property_loss']:.4f}")
    
    if patience_counter >= patience:
        print(f"\n‚èπÔ∏è Early stopping at epoch {epoch+1}")
        break

print(f"\n‚úÖ Training completed!")
print(f"üìä Best validation loss: {best_val_loss:.4f}")

# Load best model
model.load_state_dict(torch.load('models/best_vae_model.pth'))
print(f"üîÑ Loaded best model")

#%%

"""
## 5. Model Evaluation and Analysis
"""

# Evaluate model performance
def evaluate_vae(model, data):
    """Comprehensive evaluation of the trained VAE."""
    model.eval()
    
    with torch.no_grad():
        # Test set evaluation
        X_test_tensor = torch.FloatTensor(data['X_test']).to(device)
        y_test_tensor = torch.FloatTensor(data['y_test']).to(device)
        
        # Forward pass
        recon_x, predicted_property, mu, logvar, z = model(X_test_tensor)
        
        # Convert to numpy for analysis
        X_test_np = data['X_test']
        y_test_np = data['y_test']
        recon_x_np = recon_x.cpu().numpy()
        predicted_property_np = predicted_property.cpu().numpy().flatten()
        latent_codes = z.cpu().numpy()
        
        # Reconstruction metrics
        recon_mse = mean_squared_error(X_test_np, recon_x_np)
        recon_r2 = r2_score(X_test_np.flatten(), recon_x_np.flatten())
        
        # Property prediction metrics
        # Convert back to original scale
        y_test_original = data['target_scaler'].inverse_transform(y_test_np.reshape(-1, 1)).flatten()
        y_pred_original = data['target_scaler'].inverse_transform(predicted_property_np.reshape(-1, 1)).flatten()
        
        property_mse = mean_squared_error(y_test_original, y_pred_original)
        property_r2 = r2_score(y_test_original, y_pred_original)
        property_mae = np.mean(np.abs(y_test_original - y_pred_original))
        
        print(f"üìä VAE Evaluation Results:")
        print(f"\nüîÑ Reconstruction Performance:")
        print(f"   MSE: {recon_mse:.4f}")
        print(f"   R¬≤: {recon_r2:.4f}")
        
        print(f"\nüéØ Property Prediction Performance:")
        print(f"   MSE: {property_mse:.4f} (eV/atom)¬≤")
        print(f"   MAE: {property_mae:.4f} eV/atom")
        print(f"   R¬≤: {property_r2:.4f}")
        
        return {
            'recon_mse': recon_mse, 'recon_r2': recon_r2,
            'property_mse': property_mse, 'property_r2': property_r2, 'property_mae': property_mae,
            'y_test': y_test_original, 'y_pred': y_pred_original,
            'X_test': X_test_np, 'X_recon': recon_x_np,
            'latent_codes': latent_codes
        }

# Evaluate the model
results = evaluate_vae(model, data)

# Visualization of results
def plot_evaluation_results(results):
    """Plot evaluation results."""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # Property prediction scatter plot
    axes[0, 0].scatter(results['y_test'], results['y_pred'], alpha=0.6, s=20)
    min_val = min(results['y_test'].min(), results['y_pred'].min())
    max_val = max(results['y_test'].max(), results['y_pred'].max())
    axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)
    axes[0, 0].set_xlabel('True Formation Energy (eV/atom)')
    axes[0, 0].set_ylabel('Predicted Formation Energy (eV/atom)')
    axes[0, 0].set_title(f'Property Prediction (R¬≤ = {results["property_r2"]:.3f})')
    axes[0, 0].grid(True, alpha=0.3)
    
    # Prediction errors
    errors = results['y_pred'] - results['y_test']
    axes[0, 1].hist(errors, bins=50, alpha=0.7, color='orange', edgecolor='black')
    axes[0, 1].set_xlabel('Prediction Error (eV/atom)')
    axes[0, 1].set_ylabel('Frequency')
    axes[0, 1].set_title(f'Prediction Errors (MAE = {results["property_mae"]:.3f})')
    axes[0, 1].grid(True, alpha=0.3)
    
    # Feature reconstruction comparison
    feature_idx = 0  # Show first feature
    axes[0, 2].scatter(results['X_test'][:, feature_idx], results['X_recon'][:, feature_idx], 
                      alpha=0.6, s=20, color='green')
    min_val = min(results['X_test'][:, feature_idx].min(), results['X_recon'][:, feature_idx].min())
    max_val = max(results['X_test'][:, feature_idx].max(), results['X_recon'][:, feature_idx].max())
    axes[0, 2].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)
    axes[0, 2].set_xlabel(f'Original Feature {feature_idx}')
    axes[0, 2].set_ylabel(f'Reconstructed Feature {feature_idx}')
    axes[0, 2].set_title(f'Feature Reconstruction (R¬≤ = {results["recon_r2"]:.3f})')
    axes[0, 2].grid(True, alpha=0.3)
    
    # Latent space visualization (2D projection)
    latent_2d = PCA(n_components=2).fit_transform(results['latent_codes'])
    scatter = axes[1, 0].scatter(latent_2d[:, 0], latent_2d[:, 1], 
                                c=results['y_test'], cmap='viridis', alpha=0.6, s=20)
    axes[1, 0].set_xlabel('Latent Dimension 1 (PCA)')
    axes[1, 0].set_ylabel('Latent Dimension 2 (PCA)')
    axes[1, 0].set_title('Latent Space (colored by formation energy)')
    plt.colorbar(scatter, ax=axes[1, 0])
    
    # Training curves
    epochs = range(1, len(history['train_loss']) + 1)
    axes[1, 1].plot(epochs, history['train_loss'], label='Train', alpha=0.7)
    axes[1, 1].plot(epochs, history['val_loss'], label='Validation', alpha=0.7)
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Total Loss')
    axes[1, 1].set_title('Training Progress')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    # Beta annealing and loss components
    axes[1, 2].plot(epochs, history['beta'], label='Beta', linewidth=2)
    ax2 = axes[1, 2].twinx()
    ax2.plot(epochs, history['train_recon'], label='Recon Loss', alpha=0.7, color='orange')
    ax2.plot(epochs, history['train_kl'], label='KL Loss', alpha=0.7, color='green')
    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('Beta')
    ax2.set_ylabel('Loss Components')
    axes[1, 2].set_title('Beta Annealing & Loss Components')
    axes[1, 2].legend(loc='upper left')
    ax2.legend(loc='upper right')
    axes[1, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('images/vae_evaluation_results.png', dpi=300, bbox_inches='tight')
    plt.show()

    

plot_evaluation_results(results)

#%%
"""
## 6. Latent Space Analysis and Materials Discovery
"""

def analyze_latent_space(model, data, n_samples=1000):
    """Analyze the learned latent space."""
    model.eval()
    
    with torch.no_grad():
        # Encode some test samples
        X_sample = torch.FloatTensor(data['X_test'][:n_samples]).to(device)
        y_sample = data['y_test'][:n_samples]
        
        mu, logvar = model.encode(X_sample)
        z_sample = model.reparameterize(mu, logvar)
        z_np = z_sample.cpu().numpy()
        
        # Convert targets back to original scale
        y_original = data['target_scaler'].inverse_transform(y_sample.reshape(-1, 1)).flatten()
        
        print("üîç Latent Space Analysis:")
        print(f"   Latent dimension: {z_np.shape[1]}")
        print(f"   Latent mean: {z_np.mean(axis=0)}")
        print(f"   Latent std: {z_np.std(axis=0)}")
        
        # Find most and least stable materials
        most_stable_idx = np.argmin(y_original)
        least_stable_idx = np.argmax(y_original)
        
        print(f"\nüèÜ Most stable material:")
        print(f"   Formation energy: {y_original[most_stable_idx]:.3f} eV/atom")
        print(f"   Latent code: {z_np[most_stable_idx]}")
        
        print(f"\n‚ö†Ô∏è Least stable material:")
        print(f"   Formation energy: {y_original[least_stable_idx]:.3f} eV/atom")
        print(f"   Latent code: {z_np[least_stable_idx]}")
        
        return {
            'latent_codes': z_np,
            'formation_energies': y_original,
            'most_stable': (most_stable_idx, z_np[most_stable_idx], y_original[most_stable_idx]),
            'least_stable': (least_stable_idx, z_np[least_stable_idx], y_original[least_stable_idx])
        }

def decode_features_to_composition(features, feature_names, threshold=0.1):
    """
    Attempt to decode numerical features back to chemical composition.
    This is a reverse-engineering approach for educational purposes.
    """
    composition_info = {}
    
    # Look for elemental fraction features
    for i, name in enumerate(feature_names):
        if name.startswith('frac_') and len(name.split('_')) == 2:
            element = name.split('_')[1]
            fraction = features[i]
            if fraction > threshold:  # Only include significant fractions
                composition_info[element] = fraction
    
    # If no elemental fractions found, try to reconstruct from other features
    if not composition_info:
        # Look for common elements in the first part of features (if they follow the pattern)
        common_elements = ['H', 'Li', 'C', 'N', 'O', 'F', 'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'K', 'Ca', 
                          'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn']
        
        # Assume first 24 features might be elemental presence (simplified)
        for i, element in enumerate(common_elements[:min(len(common_elements), len(features))]):
            if i < len(features) and features[i] > threshold:
                composition_info[element] = features[i]
    
    return composition_info

def composition_to_formula(composition):
    """Convert composition dictionary to chemical formula string."""
    if not composition:
        return "Unknown"
    
    # Normalize to get integer ratios
    total = sum(composition.values())
    if total == 0:
        return "Unknown"
    
    # Scale to get reasonable integers
    scale_factor = 10 / total  # Scale to make total around 10
    scaled_comp = {el: val * scale_factor for el, val in composition.items()}
    
    # Sort by electronegativity order (roughly)
    element_order = ['H', 'Li', 'Na', 'K', 'Mg', 'Ca', 'Al', 'Si', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'C', 'N', 'P', 'S', 'O', 'F', 'Cl']
    
    formula_parts = []
    for element in element_order:
        if element in scaled_comp:
            ratio = scaled_comp[element]
            if ratio > 0.5:  # Only include significant amounts
                if ratio > 1.5:
                    formula_parts.append(f"{element}{ratio:.1f}")
                else:
                    formula_parts.append(element)
    
    # Add any remaining elements not in the ordered list
    for element, ratio in scaled_comp.items():
        if element not in element_order and ratio > 0.5:
            if ratio > 1.5:
                formula_parts.append(f"{element}{ratio:.1f}")
            else:
                formula_parts.append(element)
    
    return "".join(formula_parts) if formula_parts else "Unknown"

def analyze_material_identity(features, feature_names, formation_energy, material_id="Unknown"):
    """
    Analyze and identify a material from its feature vector.
    """
    # Decode to composition
    composition = decode_features_to_composition(features, feature_names)
    formula = composition_to_formula(composition)
    
    # Calculate some basic properties
    n_elements = len([el for el, frac in composition.items() if frac > 0.1])
    
    # Estimate complexity
    if n_elements <= 1:
        complexity = "Simple"
    elif n_elements <= 2:
        complexity = "Binary"
    elif n_elements <= 3:
        complexity = "Ternary"
    else:
        complexity = "Complex"
    
    # Material classification based on elements
    material_type = "Unknown"
    if composition:
        metals = {'Li', 'Na', 'K', 'Mg', 'Ca', 'Al', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn'}
        nonmetals = {'C', 'N', 'O', 'F', 'P', 'S', 'Cl'}
        
        present_metals = [el for el in composition.keys() if el in metals]
        present_nonmetals = [el for el in composition.keys() if el in nonmetals]
        
        if present_metals and present_nonmetals:
            if 'O' in present_nonmetals:
                material_type = "Oxide"
            elif 'N' in present_nonmetals:
                material_type = "Nitride"
            elif 'C' in present_nonmetals:
                material_type = "Carbide"
            else:
                material_type = "Compound"
        elif present_metals:
            material_type = "Metallic"
        elif present_nonmetals:
            material_type = "Non-metallic"
    
    return {
        'material_id': material_id,
        'formula': formula,
        'composition': composition,
        'formation_energy': formation_energy,
        'n_elements': n_elements,
        'complexity': complexity,
        'material_type': material_type,
        'features': features
    }

def generate_new_materials(model, data, n_generate=10):
    """Generate new materials by sampling from the latent space."""
    model.eval()
    
    print(f"\nüî¨ Generating {n_generate} new materials...")
    
    with torch.no_grad():
        # Sample from prior distribution N(0, I)
        z_new = torch.randn(n_generate, model.latent_dim).to(device)
        
        # Decode to get new material features
        X_new = model.decode(z_new)
        
        # Predict properties
        y_new = model.predict_property(z_new)
        
        # Convert to numpy and original scale
        X_new_np = X_new.cpu().numpy()
        y_new_original = data['target_scaler'].inverse_transform(y_new.cpu().numpy())
        
        print("üÜï Generated Materials:")
        generated_materials_info = []
        
        for i in range(n_generate):
            material_info = analyze_material_identity(
                X_new_np[i], 
                data['feature_names'], 
                y_new_original[i, 0],
                f"Generated_{i+1}"
            )
            generated_materials_info.append(material_info)
            
            print(f"   Material {i+1}: {material_info['formula']} "
                  f"({material_info['material_type']}) - "
                  f"Formation energy = {y_new_original[i, 0]:.3f} eV/atom")
        
        # Find the most promising generated material
        best_idx = np.argmin(y_new_original)
        best_material = generated_materials_info[best_idx]
        
        print(f"\n‚≠ê Most promising generated material:")
        print(f"   Formula: {best_material['formula']}")
        print(f"   Type: {best_material['material_type']} ({best_material['complexity']})")
        print(f"   Composition: {best_material['composition']}")
        print(f"   Predicted formation energy: {y_new_original[best_idx, 0]:.3f} eV/atom")
        print(f"   Latent code: {z_new[best_idx].cpu().numpy()}")
        
        return {
            'generated_features': X_new_np,
            'generated_energies': y_new_original.flatten(),
            'latent_codes': z_new.cpu().numpy(),
            'materials_info': generated_materials_info,
            'best_material': best_material
        }

def interpolate_materials(model, data, material_idx1, material_idx2, n_steps=10):
    """Interpolate between two materials in latent space."""
    model.eval()
    
    with torch.no_grad():
        # Get latent codes for two materials
        X1 = torch.FloatTensor(data['X_test'][material_idx1:material_idx1+1]).to(device)
        X2 = torch.FloatTensor(data['X_test'][material_idx2:material_idx2+1]).to(device)
        
        mu1, _ = model.encode(X1)
        mu2, _ = model.encode(X2)
        
        # Analyze the starting materials
        material1_info = analyze_material_identity(
            data['X_test'][material_idx1], 
            data['feature_names'], 
            data['target_scaler'].inverse_transform([[data['y_test'][material_idx1]]])[0,0],
            f"Material_{material_idx1}"
        )
        
        material2_info = analyze_material_identity(
            data['X_test'][material_idx2], 
            data['feature_names'], 
            data['target_scaler'].inverse_transform([[data['y_test'][material_idx2]]])[0,0],
            f"Material_{material_idx2}"
        )
        
        print(f"\nüîÑ Interpolating between materials:")
        print(f"   Start: {material1_info['formula']} ({material1_info['material_type']}) - {material1_info['formation_energy']:.3f} eV/atom")
        print(f"   End:   {material2_info['formula']} ({material2_info['material_type']}) - {material2_info['formation_energy']:.3f} eV/atom")
        print()
        
        # Linear interpolation in latent space
        alphas = np.linspace(0, 1, n_steps)
        interpolated_materials = []
        
        for i, alpha in enumerate(alphas):
            z_interp = (1 - alpha) * mu1 + alpha * mu2
            
            # Decode and predict properties
            X_interp = model.decode(z_interp)
            y_interp = model.predict_property(z_interp)
            
            # Convert to original scale
            y_original = data['target_scaler'].inverse_transform(y_interp.cpu().numpy())
            
            # Analyze interpolated material
            interp_info = analyze_material_identity(
                X_interp.cpu().numpy().flatten(),
                data['feature_names'],
                y_original[0, 0],
                f"Interpolated_{i+1}"
            )
            
            interpolated_materials.append({
                'alpha': alpha,
                'material_info': interp_info,
                'features': X_interp.cpu().numpy().flatten(),
                'formation_energy': y_original[0, 0],
                'latent_code': z_interp.cpu().numpy().flatten()
            })
            
            print(f"   Step {i+1} (Œ±={alpha:.2f}): {interp_info['formula']} "
                  f"({interp_info['material_type']}) - {y_original[0, 0]:.3f} eV/atom")
        
        return {
            'start_material': material1_info,
            'end_material': material2_info,
            'interpolated_materials': interpolated_materials
        }

# Perform latent space analysis
latent_analysis = analyze_latent_space(model, data)

# Generate new materials
generated_materials = generate_new_materials(model, data, n_generate=10)

# Interpolate between two random materials
material_idx1, material_idx2 = np.random.choice(len(data['X_test']), 2, replace=False)
interpolated = interpolate_materials(model, data, material_idx1, material_idx2, n_steps=10)

#%%
"""
## 7. Advanced VAE Applications
"""

def explore_property_space(model, data, target_energy=-3.0, search_steps=1000):
    """Search for materials with target formation energy."""
    model.eval()
    
    print(f"\nüéØ Searching for materials with formation energy ‚âà {target_energy} eV/atom...")
    
    best_materials = []
    
    with torch.no_grad():
        for step in range(search_steps):
            # Sample from latent space
            z = torch.randn(1, model.latent_dim).to(device)
            
            # Predict property
            y_pred = model.predict_property(z)
            y_original = data['target_scaler'].inverse_transform(y_pred.cpu().numpy())[0, 0]
            
            # Check if close to target
            error = abs(y_original - target_energy)
            if error < 0.5:  # Within 0.5 eV/atom
                # Decode features
                X_decoded = model.decode(z)
                
                # Analyze material
                material_info = analyze_material_identity(
                    X_decoded.cpu().numpy().flatten(),
                    data['feature_names'],
                    y_original,
                    f"Target_search_{len(best_materials)+1}"
                )
                
                best_materials.append({
                    'material_info': material_info,
                    'formation_energy': y_original,
                    'error': error,
                    'latent_code': z.cpu().numpy().flatten(),
                    'features': X_decoded.cpu().numpy().flatten()
                })
    
    print(f"üîç Found {len(best_materials)} materials within 0.5 eV/atom of target")
    
    if best_materials:
        # Sort by error
        best_materials.sort(key=lambda x: x['error'])
        
        print(f"\nüèÜ Best candidates:")
        for i, material in enumerate(best_materials[:5]):
            info = material['material_info']
            print(f"   {i+1}. {info['formula']} ({info['material_type']}) - "
                  f"Formation energy: {material['formation_energy']:.3f} eV/atom "
                  f"(error: {material['error']:.3f})")
            print(f"      Composition: {info['composition']}")
    
    return best_materials

# Search for materials with specific properties
target_materials = explore_property_space(model, data, target_energy=-3.0)

"""
## 9. Material Export and Database Functions
"""

def export_material_to_file(material_info, filename=None, format='txt'):
    """
    Export material information to a file.
    
    Args:
        material_info: Dictionary containing material information
        filename: Output filename (auto-generated if None)
        format: File format ('txt', 'json', 'cif-like')
    """
    # Ensure materials directory exists
    Path('materials').mkdir(exist_ok=True)
    
    if filename is None:
        safe_formula = material_info['formula'].replace('/', '_').replace('\\', '_')
        filename = f"materials/material_{safe_formula}_{material_info['material_id']}.{format}"
    else:
        # If filename provided, ensure it's in the materials folder
        if not filename.startswith('materials/'):
            filename = f"materials/{filename}"
    
    if format == 'txt':
        with open(filename, 'w') as f:
            f.write("# Material Information\n")
            f.write(f"Material ID: {material_info['material_id']}\n")
            f.write(f"Chemical Formula: {material_info['formula']}\n")
            f.write(f"Material Type: {material_info['material_type']}\n")
            f.write(f"Complexity: {material_info['complexity']}\n")
            f.write(f"Formation Energy: {material_info['formation_energy']:.6f} eV/atom\n")
            f.write(f"Number of Elements: {material_info['n_elements']}\n\n")
            
            f.write("# Composition (Atomic Fractions)\n")
            for element, fraction in material_info['composition'].items():
                f.write(f"{element}: {fraction:.6f}\n")
            
            f.write("\n# Feature Vector\n")
            for i, feature in enumerate(material_info['features']):
                f.write(f"Feature_{i}: {feature:.6f}\n")
    
    elif format == 'json':
        with open(filename, 'w') as f:
            # Convert numpy arrays to lists for JSON serialization
            export_data = material_info.copy()
            if 'features' in export_data:
                export_data['features'] = export_data['features'].tolist()
            json.dump(export_data, f, indent=2)
    
    elif format == 'cif-like':
        with open(filename, 'w') as f:
            f.write("# CIF-like Material Description\n")
            f.write("# This is a simplified representation for educational purposes\n")
            f.write("# Real CIF files require crystallographic data not available from VAE\n\n")
            
            f.write("data_" + material_info['formula'].replace('.', '_') + "\n\n")
            f.write("_chemical_formula_sum    '" + material_info['formula'] + "'\n")
            f.write("_chemical_name_common    '" + material_info['material_type'] + "'\n")
            f.write(f"_formation_energy        {material_info['formation_energy']:.6f}\n")
            f.write(f"_number_of_elements      {material_info['n_elements']}\n\n")
            
            f.write("# Composition\n")
            f.write("loop_\n")
            f.write("_atom_site_label\n")
            f.write("_atom_site_occupancy\n")
            for element, fraction in material_info['composition'].items():
                f.write(f"{element}1  {fraction:.6f}\n")
            
            f.write("\n# Note: Atomic coordinates would require crystal structure prediction\n")
            f.write("# which is beyond the scope of this VAE model\n")
    
    print(f"üìÅ Material exported to: {filename}")
    return filename

def create_materials_database(materials_list, filename="materials/discovered_materials.csv"):
    """
    Create a CSV database of discovered materials.
    
    Args:
        materials_list: List of material_info dictionaries
        filename: Output CSV filename
    """
    import pandas as pd
    
    # Ensure materials directory exists
    Path('materials').mkdir(exist_ok=True)
    
    # If filename doesn't start with materials/, add it
    if not filename.startswith('materials/'):
        filename = f"materials/{filename}"
    
    # Prepare data for DataFrame
    db_data = []
    for material in materials_list:
        if isinstance(material, dict) and 'material_info' in material:
            info = material['material_info']
        else:
            info = material
        
        # Extract composition as separate columns
        row = {
            'material_id': info['material_id'],
            'formula': info['formula'],
            'material_type': info['material_type'],
            'complexity': info['complexity'],
            'formation_energy': info['formation_energy'],
            'n_elements': info['n_elements']
        }
        
        # Add composition elements as columns
        for element, fraction in info['composition'].items():
            row[f'frac_{element}'] = fraction
        
        db_data.append(row)
    
    # Create DataFrame
    df = pd.DataFrame(db_data)
    
    # Fill NaN values with 0 for composition fractions
    composition_cols = [col for col in df.columns if col.startswith('frac_')]
    df[composition_cols] = df[composition_cols].fillna(0)
    
    # Save to CSV
    df.to_csv(filename, index=False)
    
    print(f"üìä Materials database created: {filename}")
    print(f"   Total materials: {len(df)}")
    print(f"   Columns: {len(df.columns)}")
    
    return df

def search_similar_materials_in_database(target_composition, database_df, similarity_threshold=0.8):
    """
    Search for similar materials in the database based on composition.
    
    Args:
        target_composition: Dictionary of element fractions
        database_df: DataFrame containing materials database
        similarity_threshold: Minimum similarity score (0-1)
    """
    # Get composition columns
    composition_cols = [col for col in database_df.columns if col.startswith('frac_')]
    
    # Create target vector
    target_vector = np.zeros(len(composition_cols))
    for i, col in enumerate(composition_cols):
        element = col.replace('frac_', '')
        target_vector[i] = target_composition.get(element, 0)
    
    # Calculate similarities
    db_compositions = database_df[composition_cols].values
    similarities = cosine_similarity([target_vector], db_compositions)[0]
    
    # Find similar materials
    similar_indices = np.where(similarities >= similarity_threshold)[0]
    
    if len(similar_indices) > 0:
        similar_materials = database_df.iloc[similar_indices].copy()
        similar_materials['similarity'] = similarities[similar_indices]
        similar_materials = similar_materials.sort_values('similarity', ascending=False)
        
        print(f"üîç Found {len(similar_materials)} similar materials (similarity ‚â• {similarity_threshold})")
        return similar_materials
    else:
        print(f"‚ùå No similar materials found with similarity ‚â• {similarity_threshold}")
        return None

# Export discovered materials
print("\nüíæ Exporting discovered materials...")

# Collect all discovered materials
all_discovered_materials = []

# Add generated materials
if 'materials_info' in generated_materials:
    all_discovered_materials.extend(generated_materials['materials_info'])

# Add target materials
for material in target_materials[:5]:  # Top 5 only
    all_discovered_materials.append(material['material_info'])

# Export individual materials
for i, material in enumerate(all_discovered_materials[:3]):  # Export first 3 as examples
    if Path('materials').exists() is False:
        Path('materials').mkdir(parents=True, exist_ok=True)
    
    print(f"Exporting material {i+1}: {material['formula']}...")
    # Path('materials').joinpath(export_material_to_file(material, format='json'))
    Path('materials').joinpath(export_material_to_file(material, format='txt'))
    Path('materials').joinpath(export_material_to_file(material, format='cif-like'))

# Create materials database
if all_discovered_materials:
    materials_db = create_materials_database(all_discovered_materials)
    
    # Example: Search for materials similar to the best generated material
    if generated_materials['materials_info']:
        best_generated = generated_materials['best_material']
        print(f"\nüîç Searching for materials similar to {best_generated['formula']}...")
        similar_materials = search_similar_materials_in_database(
            best_generated['composition'], 
            materials_db, 
            similarity_threshold=0.7
        )
        
        if similar_materials is not None:
            print("\nTop similar materials:")
            for idx, row in similar_materials.head(3).iterrows():
                print(f"   {row['formula']} (similarity: {row['similarity']:.3f}) - "
                      f"{row['formation_energy']:.3f} eV/atom")

print(f"\nüìÅ Material files and database created in organized folders:")
print(f"   üìÇ materials/ - .txt, .cif-like, and .csv files")
print(f"   üìÇ images/ - visualization dashboards")
print(f"   üìÇ models/ - trained model files")

def create_materials_visualization_dashboard(materials_list, save_filename="images/materials_dashboard.png"):
    """
    Create a comprehensive visualization dashboard for discovered materials.
    """
    if not materials_list:
        print("No materials to visualize")
        return
    
    # Ensure images directory exists
    Path('images').mkdir(exist_ok=True)
    
    # If filename doesn't start with images/, add it
    if not save_filename.startswith('images/'):
        save_filename = f"images/{save_filename}"
    
    # Prepare data for visualization
    formulas = []
    energies = []
    types = []
    complexities = []
    n_elements_list = []
    
    for material in materials_list:
        if isinstance(material, dict) and 'material_info' in material:
            info = material['material_info']
        else:
            info = material
        
        formulas.append(info['formula'])
        energies.append(info['formation_energy'])
        types.append(info['material_type'])
        complexities.append(info['complexity'])
        n_elements_list.append(info['n_elements'])
    
    # Create dashboard
    fig = plt.figure(figsize=(20, 12))
    
    # Main title
    fig.suptitle('üî¨ Discovered Materials Dashboard', fontsize=20, fontweight='bold')
    
    # 1. Formation energy distribution
    ax1 = plt.subplot(2, 4, 1)
    plt.hist(energies, bins=min(20, len(energies)), alpha=0.7, color='skyblue', edgecolor='black')
    plt.xlabel('Formation Energy (eV/atom)')
    plt.ylabel('Count')
    plt.title('Energy Distribution')
    plt.grid(True, alpha=0.3)
    
    # 2. Material types
    ax2 = plt.subplot(2, 4, 2)
    type_counts = pd.Series(types).value_counts()
    type_counts.plot(kind='bar', ax=ax2, color='lightcoral')
    plt.xlabel('Material Type')
    plt.ylabel('Count')
    plt.title('Material Types')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)
    
    # 3. Complexity distribution
    ax3 = plt.subplot(2, 4, 3)
    complexity_counts = pd.Series(complexities).value_counts()
    complexity_counts.plot(kind='pie', ax=ax3, autopct='%1.1f%%')
    plt.title('Complexity Distribution')
    
    # 4. Energy vs Number of Elements
    ax4 = plt.subplot(2, 4, 4)
    scatter = plt.scatter(n_elements_list, energies, c=energies, cmap='viridis', alpha=0.7, s=60)
    plt.xlabel('Number of Elements')
    plt.ylabel('Formation Energy (eV/atom)')
    plt.title('Energy vs Complexity')
    plt.colorbar(scatter, label='Formation Energy')
    plt.grid(True, alpha=0.3)
    
    # 5. Top 10 most stable materials
    ax5 = plt.subplot(2, 2, 3)
    # Sort by formation energy
    sorted_materials = sorted(zip(formulas, energies), key=lambda x: x[1])
    top_formulas = [x[0] for x in sorted_materials[:10]]
    top_energies = [x[1] for x in sorted_materials[:10]]
    
    y_pos = np.arange(len(top_formulas))
    bars = plt.barh(y_pos, top_energies, color='lightgreen')
    plt.yticks(y_pos, top_formulas)
    plt.xlabel('Formation Energy (eV/atom)')
    plt.title('Top 10 Most Stable Materials')
    plt.grid(True, alpha=0.3)
    
    # Add values on bars
    for i, (bar, energy) in enumerate(zip(bars, top_energies)):
        plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, 
                f'{energy:.3f}', va='center', fontsize=8)
    
    # 6. Materials summary table
    ax6 = plt.subplot(2, 2, 4)
    ax6.axis('tight')
    ax6.axis('off')
    
    # Create summary statistics
    summary_data = [
        ['Total Materials', len(materials_list)],
        ['Most Stable', f"{min(energies):.3f} eV/atom"],
        ['Least Stable', f"{max(energies):.3f} eV/atom"],
        ['Average Energy', f"{np.mean(energies):.3f} eV/atom"],
        ['Energy Std Dev', f"{np.std(energies):.3f} eV/atom"],
        ['Most Common Type', type_counts.index[0] if len(type_counts) > 0 else 'Unknown'],
        ['Binary Compounds', sum(1 for n in n_elements_list if n == 2)],
        ['Complex Materials', sum(1 for n in n_elements_list if n > 3)]
    ]
    
    table = ax6.table(cellText=summary_data, 
                     colLabels=['Property', 'Value'],
                     cellLoc='left',
                     loc='center',
                     colWidths=[0.6, 0.4])
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)
    ax6.set_title('Materials Summary', fontsize=12, fontweight='bold', pad=20)
    
    plt.tight_layout()
    plt.savefig(save_filename, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"üìä Materials dashboard saved as: {save_filename}")

def print_materials_report(materials_list):
    """
    Print a comprehensive text report of discovered materials.
    """
    print("\n" + "="*80)
    print("üî¨ DISCOVERED MATERIALS REPORT")
    print("="*80)
    
    if not materials_list:
        print("No materials discovered.")
        return
    
    # Sort materials by formation energy
    sorted_materials = sorted(materials_list, 
                            key=lambda x: x['formation_energy'] if 'formation_energy' in x 
                                        else x['material_info']['formation_energy'])
    
    print(f"\nüìä SUMMARY:")
    print(f"   Total materials discovered: {len(materials_list)}")
    
    energies = []
    types = []
    for material in materials_list:
        if isinstance(material, dict) and 'material_info' in material:
            info = material['material_info']
        else:
            info = material
        energies.append(info['formation_energy'])
        types.append(info['material_type'])
    
    print(f"   Formation energy range: {min(energies):.3f} to {max(energies):.3f} eV/atom")
    print(f"   Average formation energy: {np.mean(energies):.3f} ¬± {np.std(energies):.3f} eV/atom")
    
    # Count material types
    type_counts = pd.Series(types).value_counts()
    print(f"\nüèóÔ∏è MATERIAL TYPES:")
    for mat_type, count in type_counts.items():
        print(f"   {mat_type}: {count} materials")
    
    print(f"\nüèÜ TOP 10 MOST STABLE MATERIALS:")
    print("-" * 80)
    print(f"{'Rank':<4} {'Formula':<15} {'Type':<12} {'Energy (eV/atom)':<15} {'Composition'}")
    print("-" * 80)
    
    for i, material in enumerate(sorted_materials[:10]):
        if isinstance(material, dict) and 'material_info' in material:
            info = material['material_info']
        else:
            info = material
        
        # Format composition
        comp_str = ", ".join([f"{el}:{frac:.2f}" for el, frac in list(info['composition'].items())[:3]])
        if len(info['composition']) > 3:
            comp_str += "..."
        
        print(f"{i+1:<4} {info['formula']:<15} {info['material_type']:<12} "
              f"{info['formation_energy']:<15.3f} {comp_str}")
    
    print("\nüí° NOTES:")
    print("   - Formulas are reconstructed from feature vectors and may be approximate")
    print("   - Real materials discovery requires experimental validation")
    print("   - These are computational predictions from the VAE model")
    print("   - Consider these as starting points for further investigation")

# Create comprehensive materials analysis
if all_discovered_materials:
    print_materials_report(all_discovered_materials)
    create_materials_visualization_dashboard(all_discovered_materials)
else:
    print("‚ö†Ô∏è No materials were discovered in this run")

#%%
"""
## 8. Summary and Insights

### Key Findings:

1. **VAE Architecture**: Successfully learned a compact latent representation of materials

2. **Multi-Task Learning**: The VAE can simultaneously reconstruct materials features and predict formation energies

3. **Latent Space**: The learned latent space captures meaningful chemical relationships

4. **Material Identification**: Advanced material identification system that provides:
   - Chemical formulas reconstructed from features
   - Material type classification (Oxide, Nitride, Carbide, etc.)
   - Composition analysis with elemental fractions
   - Exportable material databases and files

### Applications Demonstrated:
- **Materials Generation**: Sample new materials from the learned distribution with chemical identities
- **Property Optimization**: Search for materials with desired properties and get their formulas
- **Materials Interpolation**: Smoothly interpolate between known materials and see chemical evolution
- **Material Export**: Export discovered materials as structured files (TXT, JSON, CIF-like format)
- **Database Creation**: Generate searchable databases of discovered materials
- **Similarity Search**: Find materials with similar compositions

### Material Discovery Capabilities:
- **Chemical Formula Reconstruction**: Decode feature vectors back to approximate chemical formulas
- **Material Classification**: Automatic classification into material types (metals, oxides, etc.)
- **Composition Analysis**: Detailed elemental composition with atomic fractions
- **File Export**: Multiple export formats for integration with other tools
- **Visual Dashboards**: Comprehensive visualization of discovered materials

### File Outputs Generated:
- Individual material files (`.txt`, `.cif-like` formats) ‚Üí `materials/` folder
- Materials database (`.csv` format) ‚Üí `materials/` folder  
- Visualization dashboard (`.png` format) ‚Üí `images/` folder
- Model files (`.pth` format) ‚Üí `models/` folder
- Comprehensive text reports

### Next Steps:
- **Advanced Architectures**: Try Œ≤-VAE, WAE, or other VAE variants
- **Larger Datasets**: Apply to real Materials Project data with full structural information
- **Multi-Property**: Extend to predict multiple materials properties
- **Integration**: Combine with reinforcement learning for active discovery
- **Experimental Validation**: Use generated formulas as starting points for synthesis
- **Crystal Structure Prediction**: Combine with structure prediction algorithms

### Important Notes:
- **Formula Accuracy**: Reconstructed formulas are approximations based on feature vectors
- **Validation Required**: All discovered materials should be validated experimentally
- **Starting Points**: Use these predictions as computational starting points for research
- **Feature Limitations**: Real chemical accuracy requires more sophisticated feature engineering

---

**üéâ Congratulations!** You've successfully implemented an advanced VAE for materials discovery 
with complete material identification capabilities! The system now provides:

‚úÖ Chemical formulas for discovered materials
‚úÖ Material type classification
‚úÖ Detailed composition analysis  
‚úÖ Exportable material databases
‚úÖ Similarity search capabilities
‚úÖ Visual analysis dashboards

This foundation can be extended to more complex materials datasets and discovery tasks with 
full chemical identification capabilities.
"""

print("\n" + "="*60)
print("üéâ VAE Materials Discovery Complete!")
print("="*60)
print(f"üìä Final Model Performance:")
print(f"   Property Prediction R¬≤: {results['property_r2']:.3f}")
print(f"   Property Prediction MAE: {results['property_mae']:.3f} eV/atom")
print(f"   Feature Reconstruction R¬≤: {results['recon_r2']:.3f}")
print(f"\nüî¨ Materials Discovery Capabilities:")
print(f"   Generated {len(generated_materials['generated_energies'])} new materials with chemical identities")
print(f"   Found {len(target_materials)} materials with target properties")
print(f"   Demonstrated latent space interpolation with chemical evolution")
print(f"   Created exportable material database with {len(all_discovered_materials)} entries")
print(f"\nüìÅ Generated Files:")
print(f"   ‚úÖ Individual material files (.txt, .cif-like formats) ‚Üí materials/")
print(f"   ‚úÖ Materials database (.csv format) ‚Üí materials/")
print(f"   ‚úÖ Visualization dashboard (.png format) ‚Üí images/")
print(f"   ‚úÖ Model files (.pth format) ‚Üí models/")
print(f"   ‚úÖ Comprehensive analysis reports")
print(f"\nüöÄ Ready for advanced materials discovery with full chemical identification!")

#%%

if __name__ == "__main__":
    print("\nüî¨ VAE Materials Discovery completed successfully!")
    print("üí° Check the generated files for detailed material information!")
    print("üß™ Use the chemical formulas as starting points for experimental validation!")